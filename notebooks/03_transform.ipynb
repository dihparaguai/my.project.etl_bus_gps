{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f13ea9",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf23ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831541b4",
   "metadata": {},
   "source": [
    "# Ambiente de Execução: Colab X VSCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usar no Google Colab\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "# path_folder_bronze = '/content/'\n",
    "# path_folder_silver = '/content/silver/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81668e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar no VSCode\n",
    "path_folder_bronze = os.path.abspath('../data/temp/bronze')\n",
    "path_folder_silver = os.path.abspath('../data/temp/silver')\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../gcp_key.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9032f5a8",
   "metadata": {},
   "source": [
    "# Leitura, filtro e limpeza dos dados\n",
    "- Objetivo: Manter os dados de chegada em cada ponto e calcular o atraso ou adiatamento em relação ao horário previsto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6fa72",
   "metadata": {},
   "source": [
    "## Informações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_month(date): # Função para extrair o ano e o mês de uma data\n",
    "    return date.strftime('%y%m')\n",
    "\n",
    "year_month = extract_year_month(pd.to_datetime('2017-06-01'))  # Exemplo de uso da função, como se fosse o dia 01 de junho de 2017\n",
    "print(f'Year-Month: {year_month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path_folder_bronze, file_name_bronze):\n",
    "    # Devido a limitação de memória, o arquivo CSV é lido em pedaços (chunks)\n",
    "    return pd.read_csv(os.path.join(path_folder_bronze, file_name_bronze), on_bad_lines='skip', chunksize=10000)\n",
    "\n",
    "file_name_bronze = f'mta_{year_month}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5581cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_first_chunk(path_folder_bronze, file_name_bronze): # Exibir o cabeçalho do primeiro pedaço (chunk) do DataFrame\n",
    "    for chunk in read_csv(path_folder_bronze, file_name_bronze):\n",
    "        display(chunk.head())\n",
    "        display(chunk.info())\n",
    "        break\n",
    "\n",
    "# Função para exibir o cabeçalho do primeiro pedaço do arquivo CSV\n",
    "show_first_chunk(path_folder_bronze, file_name_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e191820",
   "metadata": {},
   "source": [
    "## Filtro e seleção de colunas\n",
    "- Selecionar das colunas úteis\n",
    "- Remover de linhas null em Horário Programado de Chegada no Ponto\n",
    "- Filtrar apenas registros que estão no Ponto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8405356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(path_folder_bronze, file_name_bronze):\n",
    "    df_list_chunks = []  # Lista vazia para guardar os pedaços do dataset filtrado e limpo\n",
    "\n",
    "    for chunk in read_csv(path_folder_bronze, file_name_bronze):\n",
    "        df_temp = chunk[['RecordedAtTime', 'DirectionRef', 'PublishedLineName', 'VehicleRef',\n",
    "                        'ArrivalProximityText', 'ScheduledArrivalTime']]  # Colunas que serão utilizadas\n",
    "        df_temp = df_temp[(df_temp['ArrivalProximityText'] == 'at stop')] # Filtro de registros no ponto\n",
    "        df_temp.dropna(subset=['ScheduledArrivalTime'], inplace=True)  # Remoção de valores null\n",
    "        df_temp.drop(['ArrivalProximityText'], axis=1, inplace=True) # Remove a coluna do ponto pois não será usada\n",
    "        df_list_chunks.append(df_temp)\n",
    "        \n",
    "    df = pd.concat(df_list_chunks, ignore_index=True) # Concatena todos os pedaços em um único DataFrame   \n",
    "    print(f'Dados do arquivo \"{file_name_bronze}\" carregados e filtrados.') \n",
    "    return df\n",
    "    \n",
    "df = clean_and_filter_data(path_folder_bronze, file_name_bronze)\n",
    "display(df.head(15))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634c53c",
   "metadata": {},
   "source": [
    "## Adição de colunas\n",
    "- Data do Registro\n",
    "- Horário Previsto e Realizado no Ponto\n",
    "- Diferença entre Previsto e Realizado\n",
    "- Faixa Horária Prevista e Realizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_RecordedAtDate_and_Time_columns(df):\n",
    "    df['RecordedAtDate'] = pd.to_datetime(df['RecordedAtTime']).dt.date # Cria uma nova coluna com a data\n",
    "    df['RecordedAtTime'] = pd.to_datetime(df['RecordedAtTime']).dt.strftime('%H:%M:%S') # Transforma a coluna de timestamp em hora\n",
    "\n",
    "    df['RecordedAtDate'] = pd.to_datetime(df['RecordedAtDate'], format='%Y-%m-%d') # Converte a coluna de data para datetime\n",
    "    df['RecordedAtTime'] = pd.to_datetime(df['RecordedAtTime'], format='%H:%M:%S') # Converte a coluna de data e hora para datetime\n",
    "    df['RecordedAtTime'] = pd.to_timedelta(df['RecordedAtTime'].dt.strftime('%H:%M:%S')) # Extrai apenas o tempo da coluna de data e hora\n",
    "\n",
    "    print('Coluna de data e hora realizadas adicionadas.')\n",
    "    return df\n",
    "\n",
    "df = add_RecordedAtDate_and_Time_columns(df)\n",
    "display(df.head(15))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0731b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ScheduledArrivalTime(hora_str):\n",
    "    hora = int(hora_str[:2])\n",
    "    if hora > 23:\n",
    "        return \"00:\" + hora_str[3:]\n",
    "    return hora_str\n",
    "\n",
    "def fix_ScheduledArrivalTime_column(df):\n",
    "    df['ScheduledArrivalTime'] = df['ScheduledArrivalTime'].apply(fix_ScheduledArrivalTime)\n",
    "\n",
    "    df['ScheduledArrivalTime'] = pd.to_datetime(df['ScheduledArrivalTime'], format='%H:%M:%S') # Converte a coluna de data e hora para datetime\n",
    "    df['ScheduledArrivalTime'] = pd.to_timedelta(df['ScheduledArrivalTime'].dt.strftime('%H:%M:%S')) # Extrai apenas o tempo da coluna de data e hora\n",
    "    \n",
    "    print('Coluna de horário de chegada programado corrigida.')\n",
    "    return df\n",
    "\n",
    "df = fix_ScheduledArrivalTime_column(df)\n",
    "display(df.head(15))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_RecordedAtTime(each_row):\n",
    "    real = each_row['RecordedAtTime']\n",
    "    prog = each_row['ScheduledArrivalTime']\n",
    "    \n",
    "    # Se real for menor que programado e a diferença for maior que 12h, então virou o dia\n",
    "    if real < prog and (prog - real) > pd.Timedelta(hours=12):\n",
    "        return real + pd.Timedelta(days=1)\n",
    "    else:\n",
    "        return real\n",
    "\n",
    "def fix_RecordedAtTime_column(df):\n",
    "    df['RecordedAtTime'] = df.apply(fix_RecordedAtTime, axis=1)\n",
    "    \n",
    "    print('Intervalo de dias em horários realizados após a meia-noite ajustado para o dia seguinte com base no horário programado.')\n",
    "    return df\n",
    "\n",
    "df = fix_RecordedAtTime_column(df)\n",
    "display(df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_DiffArrivalMins_column(df): # Função para adicionar a coluna de diferença entre a chegada programada e a chegada real\n",
    "    df['DiffArrivalMins'] = (df['RecordedAtTime'] - df['ScheduledArrivalTime']).dt.total_seconds()/60 # Converte a diferença de tempo para minutos\n",
    "    df['DiffArrivalMins'] = df['DiffArrivalMins'].astype('int64')  # Converte a diferença de tempo para minutos\n",
    "    \n",
    "    print('Coluna de entre a chegada programada e a chegada real adicionada.')\n",
    "    return df\n",
    "\n",
    "df = add_DiffArrivalMins_column(df)\n",
    "display(df.sample(15))\n",
    "display(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Recorded_and_ScheduledTimeRange_columns(df): # Função para adicionar colunas de faixa horária do horário registrado e programado\n",
    "    df['RecordedTimeRange'] = df['RecordedAtTime'].dt.components['hours'].astype('int64') # Extrai apenas a hora da coluna de horário registrado\n",
    "    df['ScheduledTimeRange'] = df['ScheduledArrivalTime'].dt.components['hours'].astype('int64') # Extrai apenas a hora da coluna de horário programado\n",
    "\n",
    "    print(\"Coluna de faixa horária do registrado e programado adicionadas.\")\n",
    "    return df\n",
    "\n",
    "df = add_Recorded_and_ScheduledTimeRange_columns(df)\n",
    "display(df[df['RecordedTimeRange'] != df['ScheduledTimeRange']].sample(15)) # Exibe amostra de registros onde a faixa horária do registrado é diferente do programado\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85b5b2",
   "metadata": {},
   "source": [
    "## Correção de Tipos das demais colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db356f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_types(df): # Função para alterar os tipos de dados das colunas do DataFrame\n",
    "    df = df.astype({'DirectionRef': 'int64',  # Converte algumas colunas para o tipo certo\n",
    "                    'PublishedLineName': 'string',\n",
    "                    'VehicleRef': 'string'})\n",
    "\n",
    "    print(\"Tipos de dados das colunas restantes alterados.\")\n",
    "    return df\n",
    "\n",
    "df = change_data_types(df)\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8546a",
   "metadata": {},
   "source": [
    "# Salvamento dos dados transformados e upload pra Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01696b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_silver = f\"mta_{year_month}_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv_silver(df, path_folder_silver, file_name_silver): # Função para salvar o DataFrame limpo no diretório silver\n",
    "    df.to_csv(os.path.join(path_folder_silver, file_name_silver), index=False) # Salva o DataFrame limpo no silver\n",
    "\n",
    "    print(f'Arquivo \"{file_name_silver}\" salvo no diretório \"silver/\".')\n",
    "\n",
    "save_to_csv_silver(df, path_folder_silver, file_name_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bucket_silver(path_folder_silver, file_name_silver): # Função para enviar o arquivo limpo para o bucket do GCP\n",
    "    client = storage.Client() # Cria o cliente para acessar o bucket\n",
    "    bucket = client.bucket('etl_bus_gps') # Referência pro bucket do GCP\n",
    "    \n",
    "    path_folder_file_silver = os.path.join(path_folder_silver, file_name_silver) # Caminho completo do arquivo a ser enviado\n",
    "    \n",
    "    blob = bucket.blob(f'temp/silver/{file_name_silver}') # Cria o blob no bucket\n",
    "    blob.upload_from_filename(path_folder_file_silver) # Faz o upload do arquivo para o bucket\n",
    "\n",
    "    print(f'Arquivo \"{file_name_silver}\" enviado para a pasta \"silver/\" no bucket \"{bucket.name}\"')\n",
    "\n",
    "upload_to_bucket_silver(path_folder_silver, file_name_silver) # Envia o arquivo limpo para o bucket do GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b988d8f",
   "metadata": {},
   "source": [
    "# Pipeline de Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c70a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_silver(path_folder_bronze, file_name_bronze, path_folder_silver, file_name_silver): # Função que executa todo o pipeline de limpeza, transformação, salvamento e upload\n",
    "    print(\"Iniciando o pipeline Silver...\")\n",
    "    df = clean_and_filter_data(path_folder_bronze, file_name_bronze)  # Limpa e filtra os dados\n",
    "    df = add_RecordedAtDate_and_Time_columns(df)  # Adiciona colunas de data e de hora\n",
    "    df = fix_ScheduledArrivalTime_column(df)  # Corrige a coluna de horário programado\n",
    "    df = fix_RecordedAtTime_column(df)  # Corrige a parte de dias do intervalo de horário registrado\n",
    "    df = add_DiffArrivalMins_column(df)  # Adiciona coluna de diferença de tempo entre horário registrado e programado\n",
    "    df = add_Recorded_and_ScheduledTimeRange_columns(df)  # Adiciona colunas de faixa horária para horário registrado e programado\n",
    "    df = change_data_types(df)  # Altera os tipos de dados das colunas restantes\n",
    "    save_to_csv_silver(df, path_folder_silver, file_name_silver)  # Salva o DataFrame limpo no silver\n",
    "    upload_to_bucket_silver(path_folder_silver, file_name_silver)  # Envia o arquivo limpo para o bucket do GCP\n",
    "\n",
    "    print(f'Pipeline Silver executado com sucesso para o arquivo \"{file_name_bronze}\"!!\\n')\n",
    "    \n",
    "pipeline_silver(path_folder_bronze, file_name_bronze, path_folder_silver, file_name_silver)  # Executa o pipeline completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dcf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_list = [] # Lista para armazenar os anos e meses\n",
    "\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-06-01'))) # Adiciona o ano e mês de junho de 2017\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-08-01'))) # Adiciona o ano e mês de agosto de 2017\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-10-01'))) # Adiciona o ano e mês de outubro de 2017\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-12-01'))) # Adiciona o ano e mês de dezembro de 2017\n",
    "\n",
    "for year_month in year_month_list: # Itera sobre os anos e meses na lista, simulando o Airflow\n",
    "    file_name_bronze = f'mta_{year_month}.csv'  # Define o nome do arquivo bronze com base no ano e mês\n",
    "    file_name_silver = f\"mta_{year_month}_cleaned.csv\"  # Define o nome do arquivo silver com base no ano e mês\n",
    "    pipeline_silver(path_folder_bronze, file_name_bronze, path_folder_silver, file_name_silver)  # Executa o pipeline para o ano e mês especificados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
