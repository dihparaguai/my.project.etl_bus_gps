{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c1ee24",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceace0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4903d32",
   "metadata": {},
   "source": [
    "# Ambiente de Execução: Colab X VSCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d92927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usar no Google Colab\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "# path_folder_silver = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar no VSCode\n",
    "path_folder_silver = os.path.abspath('../data/temp/silver')\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../gcp_key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab18182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_month(date): # Função para extrair o ano e o mês de uma data\n",
    "    return date.strftime('%y%m')\n",
    "\n",
    "year_month = extract_year_month(pd.to_datetime('2017-06-01'))  # Exemplo de uso da função, como se fosse o dia 01 de junho de 2017\n",
    "print(f'Year-Month: {year_month}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867ea0d",
   "metadata": {},
   "source": [
    "# Leitura dos Dados Limpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d08ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_cleaned(path_folder_silver, file_name_silver): # Função para ler o arquivo CSV limpo\n",
    "    df = pd.read_csv(os.path.join(path_folder_silver, file_name_silver))\n",
    "    \n",
    "    print(f'Arquivo \"{file_name_silver}\" lido.')\n",
    "    return df\n",
    "\n",
    "file_name_silver = f'mta_{year_month}_cleaned.csv'\n",
    "\n",
    "df = read_csv_cleaned(path_folder_silver, file_name_silver)\n",
    "display(df.head(15))\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e2382",
   "metadata": {},
   "source": [
    "# Ajustes iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e629bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_types(df): # Função para alterar os tipos de dados das colunas do DataFrame\n",
    "    df = df.astype({'PublishedLineName': 'string', 'VehicleRef': 'string'}) # Converte as colunas PublishedLineName e VehicleRef para string\n",
    "    df['RecordedAtDate'] = pd.to_datetime(df['RecordedAtDate'])  # Converte a coluna RecordedAtDate para datetime\n",
    "    df['RecordedAtTime'] = pd.to_timedelta(df['RecordedAtTime'])  # Converte a coluna RecordedAtTime para timedelta\n",
    "    df['ScheduledArrivalTime'] = pd.to_timedelta(df['ScheduledArrivalTime'])  # Converte a coluna ScheduledArrivalTime para timedelta\n",
    "    \n",
    "    print(\"Tipos de dados das colunas alterados.\")\n",
    "    return df\n",
    "\n",
    "df = change_data_types(df)\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1e5ca",
   "metadata": {},
   "source": [
    "# Criação / atualização de arquivos dimensão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_gold_dim = os.path.abspath(f'../data/temp/gold/dim/') # Caminho para a pasta de saída dos arquivos dimensionais\n",
    "\n",
    "file_name_dim_PublishedLine = f'dim_mta_PublishedLine.csv' # Nome do arquivo para a dimensão PublishedLine\n",
    "file_name_dim_VehicleRef = f'dim_mta_VehicleRef.csv' # Nome do arquivo para a dimensão VehicleRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dim(path_folder_gold_dim, file_name_dim): # Função para ler o arquivo CSV da dimensão PublishedLine já existente\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(path_folder_gold_dim, file_name_dim))  # Tenta ler o arquivo CSV da dimensão\n",
    "        print(f'Arquivo \"{file_name_dim}\" lido.')\n",
    "        return df\n",
    "    except:\n",
    "        print(f'Arquivo \"{file_name_dim}\" não encontrado.')\n",
    "        return pd.DataFrame()  # se falhar, cria um DataFrame vazio\n",
    "\n",
    "\n",
    "df_dim_PublishedLineName = read_dim(path_folder_gold_dim, file_name_dim_PublishedLine)\n",
    "df_dim_VehicleRef = read_dim(path_folder_gold_dim, file_name_dim_VehicleRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa688b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dim_PublishedLineName(df_new, df_dim): # Função para criar ou atualizar uma Dimensão com a coluna PublishedLineName\n",
    "    df_new = df_new[['PublishedLineName']].drop_duplicates() # O duplo colchete é necessário para manter o nome da coluna original\n",
    "    df_dim = pd.concat([df_new, df_dim], ignore_index=True).drop_duplicates()  # Concatena o DataFrame novo com o Dimensão existente\n",
    "    \n",
    "    print(\"Dimensão 'PublishedLineName' criado/atualizado.\")\n",
    "    return df_dim\n",
    "\n",
    "df_dim_PublishedLineName = update_dim_PublishedLineName(df, df_dim_PublishedLineName)\n",
    "display(df_dim_PublishedLineName.head(15))\n",
    "display(df_dim_PublishedLineName.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e723937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dim_VehicleRef(df_new, df_dim): # Função para criar um Dimensão com a coluna VehicleRef\n",
    "    df_new = df_new[['VehicleRef']].drop_duplicates() # O duplo colchete é necessário para manter o nome da coluna original\n",
    "    df_dim = pd.concat([df_new, df_dim], ignore_index=True).drop_duplicates()  # Concatena o DataFrame novo com o Dimensão existente\n",
    "    \n",
    "    print(\"Dimensão 'VehicleRef' criado/atualizado.\")\n",
    "    return df_dim\n",
    "\n",
    "df_dim_VehicleRef = update_dim_VehicleRef(df, df_dim_VehicleRef) # Usa um DataFrame vazio como Dimensão inicial\n",
    "display(df_dim_VehicleRef.head(15))\n",
    "display(df_dim_VehicleRef.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6f657",
   "metadata": {},
   "source": [
    "# Exportação para CSV e envio para Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder_gold_fact = os.path.abspath(f'../data/temp/gold/fact/') # Caminho para a pasta de saída dos arquivos factuais\n",
    "\n",
    "file_name_fact = f'fact_mta_{year_month}.csv' # Nome do arquivo para a tabela factual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv_gold(path_folder_gold, file_name_gold, df): # Função para salvar o DataFrame em um arquivo CSV na pasta Gold\n",
    "    os.makedirs(path_folder_gold, exist_ok=True)  # Cria a pasta Gold se ela não existir\n",
    "\n",
    "    df.to_csv(os.path.join(path_folder_gold, file_name_gold), index=False) # Salva o DataFrame em um arquivo CSV na pasta Gold\n",
    "    \n",
    "    print(f'Arquivo \"{file_name_gold}\" salvo na pasta \"{path_folder_gold}\".')\n",
    "\n",
    "save_to_csv_gold(path_folder_gold_dim, file_name_dim_PublishedLine, df_dim_PublishedLineName)\n",
    "save_to_csv_gold(path_folder_gold_dim, file_name_dim_VehicleRef, df_dim_VehicleRef)\n",
    "save_to_csv_gold(path_folder_gold_fact, file_name_fact, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bucket_gold(path_folder_gold, file_name_gold): # Função para fazer o upload do arquivo CSV para o bucket do GCP\n",
    "    client = storage.Client() # Cria o cliente para acessar o bucket\n",
    "    bucket = client.bucket('etl_bus_gps') # Referência pro bucket do GCP\n",
    "    \n",
    "    path_folder_file_gold = os.path.join(path_folder_gold, file_name_gold) # Caminho completo do arquivo junto com o nome do arquivo\n",
    "    \n",
    "    folder_type = 'fact' if 'fact' in path_folder_gold else 'dim'  # Determina o tipo de pasta (fact ou dim) com base no caminho do arquivo, para que o arquivo seja enviado para a pasta correta no bucket\n",
    "    \n",
    "    blob = bucket.blob(f'temp/gold/{folder_type}/{file_name_gold}') # Cria o blob no bucket\n",
    "    blob.upload_from_filename(path_folder_file_gold) # Faz o upload do arquivo para o bucket\n",
    "    \n",
    "    print(f'Arquivo \"{file_name_gold}\" enviado para a pasta \"{blob}\" no bucket \"{bucket.name}\"')\n",
    "\n",
    "upload_to_bucket_gold(path_folder_gold_dim, file_name_dim_PublishedLine)\n",
    "upload_to_bucket_gold(path_folder_gold_dim, file_name_dim_VehicleRef) \n",
    "upload_to_bucket_gold(path_folder_gold_fact, file_name_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b1333",
   "metadata": {},
   "source": [
    "# Pipeline de Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eea543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_gold(year_month): # Função para executar todas as funções de manipulação e upload dos dados\n",
    "    \n",
    "    path_folder_silver = os.path.abspath('../data/temp/silver') # Caminho para a pasta de entrada dos arquivos limpos\n",
    "    file_name_silver = f\"mta_{year_month}_cleaned.csv\" # Nome do arquivo limpo a ser lido\n",
    "    \n",
    "    path_folder_gold_fact = os.path.abspath(f'../data/temp/gold/fact/') # Caminho para a pasta de saída dos arquivos factuais\n",
    "    file_name_fact = f'fact_mta_{year_month}.csv' # Nome do arquivo para a tabela factual\n",
    "    \n",
    "    path_folder_gold_dim = os.path.abspath(f'../data/temp/gold/dim/') # Caminho para a pasta de saída dos arquivos dimensionais\n",
    "    file_name_dim_PublishedLine = f'dim_mta_PublishedLine.csv' # Nome do arquivo para a dimensão PublishedLine\n",
    "    file_name_dim_VehicleRef = f'dim_mta_VehicleRef.csv' # Nome do arquivo para a dimensão VehicleRef\n",
    "    \n",
    "    print('Iniciando o pipeline Gold...')\n",
    "    df = read_csv_cleaned(path_folder_silver, file_name_silver) # Lê o arquivo CSV limpo\n",
    "    df = change_data_types(df) # Altera os tipos de dados das colunas do DataFrame\n",
    "    \n",
    "    df_dim_PublishedLineName = read_dim(path_folder_gold_dim, file_name_dim_PublishedLine) # Lê o arquivo CSV da dimensão PublishedLine já existente\n",
    "    df_dim_PublishedLineName = update_dim_PublishedLineName(df, df_dim_PublishedLineName) # Atualiza ou cria a Dimensão PublishedLineName\n",
    "    df_dim_VehicleRef = read_dim(path_folder_gold_dim, file_name_dim_VehicleRef) # Lê o arquivo CSV da dimensão VehicleRef já existente\n",
    "    df_dim_VehicleRef = update_dim_VehicleRef(df, df_dim_VehicleRef) # Atualiza ou cria a Dimensão VehicleRef\n",
    "    \n",
    "    data_to_save = { # Dicionário para armazenar: caminhos da pasta de saída, nomes dos arquivos e DataFrames a serem salvos\n",
    "        path_folder_gold_dim: {file_name_dim_PublishedLine: df_dim_PublishedLineName, file_name_dim_VehicleRef: df_dim_VehicleRef},\n",
    "        path_folder_gold_fact: {file_name_fact: df}\n",
    "    }\n",
    "    \n",
    "    for path_folder_gold, files_names_gold in data_to_save.items(): # Itera sobre os caminhos da pasta de saída, nomes dos arquivos e DataFrames a serem salvos\n",
    "        for file_name_gold, df in files_names_gold.items():\n",
    "            save_to_csv_gold(path_folder_gold, file_name_gold, df) # Salva o DataFrame em um arquivo CSV na pasta Gold\n",
    "            upload_to_bucket_gold(path_folder_gold, file_name_gold) # Faz o upload do arquivo CSV para o bucket do GCP\n",
    "    \n",
    "    print(f'Pipeline Gold concluído para o arquivo \"{file_name_silver}\"!!\\n')\n",
    "    \n",
    "pipeline_gold(year_month)  # Executa o pipeline Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123bab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month_list = [] # Lista para armazenar os anos e meses\n",
    "\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-06-01'))) # Adiciona o ano e mês de junho de 2017\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-08-01'))) # Adiciona o ano e mês de agosto de 2017\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-10-01'))) # Adiciona o ano e mês de outubro de 2017\n",
    "year_month_list.append(extract_year_month(pd.to_datetime('2017-12-01'))) # Adiciona o ano e mês de dezembro de 2017\n",
    "\n",
    "for year_month in year_month_list: # Itera sobre os anos e meses na lista, simulando o Airflow\n",
    "    pipeline_gold(year_month)  # Executa o pipeline para o ano e mês especificados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
